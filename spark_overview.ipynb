{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIBzgskbObVfpAn/qRStmO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julihdez36/Analytics/blob/main/spark_overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Qué es PySpark?\n",
        "\n",
        "PySpark es una API de Python para trabajar con Apache Spark. Primero explicaré qué quiero decir con una \"API de Python\" para algo y luego explicaré qué es, específicamente, \"Apache Spark\".\n",
        "\n",
        "Lo que quiero decir con \"API de Python\" es que puedes usar la sintaxis y la agilidad de Python para interactuar y enviar comandos a un sistema que no está basado, en esencia, en Python.\n",
        "\n",
        "Con PySpark, interactúas con Apache Spark, un sistema diseñado para trabajar, analizar y modelar con inmensas cantidades de datos en muchas computadoras al mismo tiempo. Dicho de otra manera, Apache Spark te permite ejecutar cálculos en paralelo, en lugar de secuencialmente. Te permite dividir una tarea increíblemente grande en muchas tareas más pequeñas y ejecutar cada una de esas tareas en una máquina diferente. Esto te permite lograr tus objetivos de análisis en un tiempo razonable que no sería posible en una sola máquina.\n",
        "\n",
        "Generalmente, definiríamos la cantidad de datos que se adapta a PySpark como lo que no cabe en el almacenamiento de una sola máquina (y mucho menos en la RAM).\n",
        "\n",
        "## Conceptos relacionados importantes:\n",
        "\n",
        "1. Computación distribuida: cuando se distribuye una tarea en varias tareas más pequeñas que se ejecutan al mismo tiempo. Esto es lo que PySpark le permite hacer con muchas máquinas, pero también se puede hacer en una sola máquina con varios subprocesos, por ejemplo.\n",
        "2. Clúster: una red de máquinas que pueden asumir tareas de un usuario, interactuar entre sí y devolver resultados. Estos proporcionan los recursos informáticos que PySpark utilizará para realizar los cálculos.\n",
        "3. Conjunto de datos distribuidos resilientes (RDD): una colección de datos distribuida e inmutable. No es tabular, como DataFrames con los que trabajaremos más adelante, y no tiene un esquema de datos. Por lo tanto, para la manipulación de datos tabulares, DataFrames permite más opciones de API y optimizaciones poco conocidas. Aun así, es posible que se encuentre con RDD a medida que aprenda más sobre Spark, y debe estar al tanto de su existencia.\n",
        "\n",
        "## Parte de PySpark que cubriremos:\n",
        "\n",
        "1. PySpark SQL: contiene comandos para el procesamiento y la manipulación de datos.\n",
        "2. PySpark MLlib: incluye una variedad de modelos, entrenamiento de modelos y comandos relacionados.\n",
        "\n",
        "**Arquitectura Spark:** para enviar comandos y recibir resultados de un clúster, deberá iniciar una sesión Spark. Este objeto es su herramienta para interactuar con Spark. Cada usuario del clúster tendrá su propia sesión Spark, que le permitirá usar el clúster de forma aislada de otros usuarios. Todas las sesiones se comunican con un contexto Spark, que es el nodo maestro del clúster; es decir, asigna tareas a cada una de las computadoras del clúster y las coordina. Cada una de las computadoras del clúster que realiza tareas para un nodo maestro se denomina nodo de trabajo. Para conectarse a un nodo de trabajo, el nodo maestro debe obtener la potencia de cómputo de ese nodo asignada por un administrador de clúster, que es responsable de distribuir los recursos del clúster. Dentro de cada nodo de trabajo, hay programas de ejecución que ejecutan las tareas; pueden ejecutar múltiples tareas simultáneamente y tienen su propio caché para almacenar resultados. Por lo tanto, cada nodo maestro puede tener múltiples nodos de trabajo, que pueden tener múltiples tareas en ejecución."
      ],
      "metadata": {
        "id": "3d8IzEpsJKdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a SparkSession object can perform the most common data processing tasks\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('test').getOrCreate() # will return existing session if one was\n",
        "                                                           # created before and was not closed"
      ],
      "metadata": {
        "id": "sTt26PnzKGW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "-U3zcSGNKHSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset: https://www.kaggle.com/fedesoriano/heart-failure-prediction"
      ],
      "metadata": {
        "id": "OVR0WqcqKN-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read csv, all columns will be of type string\n",
        "df = spark.read.option('header','true').csv('heart.csv')\n",
        "# Indíquele a pyspark el tipo de columnas: ahorra tiempo en conjuntos de datos grandes. Hay otras formas de hacer esto, pero esa es mi favorita\n",
        "\n",
        "schema = 'Age INTEGER, Sex STRING, ChestPainType STRING'\n",
        "df = spark.read.csv('/Users/mreznik/heart.csv', schema=schema, header=True)\n",
        "# let PySpark infer the schema\n",
        "df = spark.read.csv('/Users/mreznik/heart.csv', inferSchema=True, header=True)\n",
        "# replace nulls with other value at reading time\n",
        "df = spark.read.csv('/Users/mreznik/heart.csv', nullValue='NA')\n",
        "# save data\n",
        "df.write.format(\"csv\").save(\"heart_save.csv\")\n",
        "# if you want to overwrite the file\n",
        "df.write.format(\"csv\").mode(\"overwrite\").save(\"heart_save.csv\")"
      ],
      "metadata": {
        "id": "ftn-edvUKJpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show head of table\n",
        "df.show(3)"
      ],
      "metadata": {
        "id": "yMQW-svaKJtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count number of rows\n",
        "df.count()"
      ],
      "metadata": {
        "id": "WThaFzNeKJxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show parts of the table\n",
        "df.select('Age').show(3)\n",
        "df.select(['Age','Sex']).show(3)"
      ],
      "metadata": {
        "id": "uK7FS6SIKJz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pandas DataFrame VS PySpark DataFrame\n",
        "\n",
        "Ambos representan una tabla de datos con filas y columnas. Sin embargo, en esencia son diferentes, ya que el marco de datos de PySpark debe admitir cálculos distribuidos. A medida que avancemos, veremos cada vez más características que no están presentes en Pandas DataFrame. Dicho esto, si sabe cómo usar Pandas, entonces pasarse a PySpark se sentirá como una transición natural.\n",
        "\n",
        "## DAG\n",
        "\n",
        "El gráfico acíclico dirigido es la forma en que Spark ejecuta los cálculos. Cuando le da una serie de transformaciones para aplicar al conjunto de datos, crea un gráfico a partir de esas transformaciones, por lo que sabe qué hacer, pero no ejecuta esos comandos inmediatamente, si no es necesario. En cambio, es perezoso: pasará por el DAG y aplicará las transformaciones solo cuando sea necesario, para proporcionar un resultado necesario. Esto permite un mejor rendimiento, ya que Spark sabe qué hay por delante de un determinado cálculo y optimiza el proceso en consecuencia.\n",
        "\n",
        "## Transformaciones VS acciones\n",
        "\n",
        "En PySpark, hay dos tipos de comandos: transformaciones y acciones. Los comandos de transformación se agregan al DAG, pero no hacen que se ejecute. Transforman un DataFrame en otro, sin cambiar el DataFrame de entrada. Por otro lado, las acciones hacen que PySpark ejecute el DAG, pero no crean un nuevo DataFrame; en cambio, generan el resultado del DAG.\n",
        "\n",
        "## Almacenamiento en caché\n",
        "\n",
        "Cada vez que ejecuta un DAG, se volverá a calcular desde el principio. Es decir, los resultados no se guardan en la memoria. Por lo tanto, si queremos guardar un resultado para que no tenga que volver a calcularse, podemos usar el comando de caché. Tenga en cuenta que esto ocupará espacio en la memoria del nodo de trabajo, así que tenga cuidado con los tamaños de los conjuntos de datos que está almacenando en caché. De manera predeterminada, el DF en caché se almacena en la RAM y no se serializa (no se convierte en un flujo de bytes). Puede cambiar ambos: almacenar los datos en el disco duro, serializarlos o ambos.\n",
        "\n",
        "## Recopilación\n",
        "\n",
        "Incluso después de almacenar en caché un DataFrame, aún permanece en la memoria de los nodos de trabajo. Si desea recopilar sus piezas, ensamblarlas y guardarlas en el nodo maestro para no tener que extraerlas cada vez, use el comando para recopilar. Nuevamente, tenga mucho cuidado con esto, ya que el archivo recopilado tendrá que caber en la memoria del nodo maestro."
      ],
      "metadata": {
        "id": "dS6noJyBKiOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.cache()\n",
        "df.collect()"
      ],
      "metadata": {
        "id": "U2ERycrNKJ3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert PySpark DataFrame to Pandas DataFrame\n",
        "pd_df = df.toPandas()\n",
        "# convert it back\n",
        "spark_df = spark.createDataFrame(pd_df)"
      ],
      "metadata": {
        "id": "tY7IofYSKJ6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show first three rows as three row objects, which is how spark represents single rows from a table.\n",
        "# we will learn more about it later\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "E3C9fmxcKxiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type os columns\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "kbeqYs4TKxqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# column dtypes as list of tuples\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "HWu7WNvvKxs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cast a column from one type to other\n",
        "from pyspark.sql.types import FloatType\n",
        "df = df.withColumn(\"Age\",df.Age.cast(FloatType()))\n",
        "df = df.withColumn(\"RestingBP\",df.Age.cast(FloatType()))"
      ],
      "metadata": {
        "id": "Dy9dGkuTKxwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute summery statistics\n",
        "df.select(['Age','RestingBP']).describe().show()"
      ],
      "metadata": {
        "id": "QvX59j6LLDbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add a new column or replace existing one\n",
        "AgeFixed = df['Age'] + 1  # select alwayes returns a DataFrame object, and we need a column object\n",
        "df = df.withColumn('AgeFixed', AgeFixed)"
      ],
      "metadata": {
        "id": "Rv2xE7NZLDk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(['AgeFixed','Age']).describe().show()\n"
      ],
      "metadata": {
        "id": "sQHiH3NcLDoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove columns\n",
        "df.drop('AgeFixed').show(1) # add df = to get the new DataFrame into a variable"
      ],
      "metadata": {
        "id": "LSkKKsxALDrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rename a column\n",
        "df.withColumnRenamed('Age','age').select('age').show(1)\n",
        "# to rename more than a single column, i would suggest a loop.\n",
        "name_pairs = [('Age','age'),('Sex','sex')]\n",
        "for old_name, new_name in name_pairs:\n",
        "    df = df.withColumnRenamed(old_name,new_name)"
      ],
      "metadata": {
        "id": "4Pt3n906Kxyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(['age','sex']).show(1)\n"
      ],
      "metadata": {
        "id": "1zYrpe6HKx1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop all rows that contain any NA\n",
        "df = df.na.drop()\n",
        "df.count()\n",
        "# drop all rows where all values are NA\n",
        "df = df.na.drop(how='all')\n",
        "# drop all rows where more at least 2 values are NOT NA\n",
        "df = df.na.drop(thresh=2)\n",
        "# drop all rows where any value at specific columns are NAs.\n",
        "df = df.na.drop(how='any', subset=['age','sex']) # 'any' is the defult"
      ],
      "metadata": {
        "id": "AYuht8_5Kx4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill missing values in a specific column with a '?'\n",
        "df = df.na.fill(value='?',subset=['sex'])\n",
        "# replace NAs with mean of column\n",
        "from pyspark.ml.feature import Imputer # In statistics, imputation is the process of\n",
        "                                       # replacing missing data with substituted values\n",
        "imptr = Imputer(inputCols=['age','RestingBP'],\n",
        "                outputCols=['age','RestingBP']).setStrategy('mean') # can also be 'median' and so on\n",
        "\n",
        "df = imptr.fit(df).transform(df)"
      ],
      "metadata": {
        "id": "PRYjWKmoKx62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter to adults only and calculate mean\n",
        "df.filter('age > 18')\n",
        "df.where('age > 18')# 'where' is an alias to 'filter'\n",
        "df.where(df['age'] > 18) # third option\n",
        "# add another condition ('&' means and, '|' means or)\n",
        "df.where((df['age'] > 18) | (df['ChestPainType'] == 'ATA'))\n",
        "# take every record where the 'ChestPainType' is NOT 'ATA'\n",
        "df.filter(~(df['ChestPainType'] == 'ATA'))"
      ],
      "metadata": {
        "id": "2keYaklsLShC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter('age > 18').show()"
      ],
      "metadata": {
        "id": "6CAafP_LLSnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate a string expression into command\n",
        "from pyspark.sql.functions import expr\n",
        "exp = 'age + 0.2 * AgeFixed'\n",
        "df.withColumn('new_col', expr(exp)).select('new_col').show(3)"
      ],
      "metadata": {
        "id": "b7Re0LqGLSqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# group by age\n",
        "disease_by_age = df.groupby('age').mean().select(['age','avg(HeartDisease)'])\n",
        "# sort values in desnding order\n",
        "from pyspark.sql.functions import desc\n",
        "disease_by_age.orderBy(desc(\"age\")).show(5)"
      ],
      "metadata": {
        "id": "LZDQMUnnLStU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import asc\n",
        "disease_by_age = df.groupby('age').mean().select(['age','avg(HeartDisease)'])\n",
        "disease_by_age.orderBy(desc(\"age\")).show(3)"
      ],
      "metadata": {
        "id": "Q2sY3L7GLSwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aggregate to get several statistics for several columns\n",
        "# the available aggregate functions are avg, max, min, sum, count\n",
        "from pyspark.sql import functions as F\n",
        "df.agg(F.min(df['age']),F.max(df['age']),F.avg(df['sex'])).show()"
      ],
      "metadata": {
        "id": "VcICjC1cLbfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('HeartDisease').agg(F.min(df['age']),F.avg(df['sex'])).show()\n"
      ],
      "metadata": {
        "id": "2LKdu90RLbjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run an SQL query on the data\n",
        "df.createOrReplaceTempView(\"df\") # tell PySpark how the table will be called in the SQL query\n",
        "spark.sql(\"\"\"SELECT sex from df\"\"\").show(2)\n",
        "\n",
        "# we also choose columns using SQL sytnx, with a command that combins '.select()' and '.sql()'\n",
        "df.selectExpr(\"age >= 40 as older\", \"age\").show(2)"
      ],
      "metadata": {
        "id": "21pvNRVuLbmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('age').pivot('sex', (\"M\", \"F\")).count().show(3)"
      ],
      "metadata": {
        "id": "qRWABMV9LbpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pivot - expensive operation\n",
        "df.selectExpr(\"age >= 40 as older\", \"age\",'sex').groupBy(\"sex\")\\\n",
        "                    .pivot(\"older\", (\"true\", \"false\")).count().show()"
      ],
      "metadata": {
        "id": "Gc73H0pSLjO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(['age','MaxHR','Cholesterol']).show(4)"
      ],
      "metadata": {
        "id": "70e6J_Q4LjXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# devide dataset to training features and target\n",
        "X_column_names = ['Age','Cholesterol']\n",
        "target_colum_name = ['MaxHR']\n",
        "\n",
        "# convert feature columns into a columns where the vlues are feature vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "v_asmblr = VectorAssembler(inputCols=X_column_names, outputCol='Fvec')\n",
        "df = v_asmblr.transform(df)\n",
        "X = df.select(['Age','Cholesterol','Fvec','MaxHR'])\n",
        "X.show(3)"
      ],
      "metadata": {
        "id": "AF_UnQSLLqCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# devide dataset into training and testing sets\n",
        "trainset, testset = X.randomSplit([0.8,0.2])"
      ],
      "metadata": {
        "id": "igqQnZL6LqIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict 'RestingBP' using linear regression\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "model = LinearRegression(featuresCol='Fvec', labelCol='MaxHR')\n",
        "model = model.fit(trainset)\n",
        "print(model.coefficients)\n",
        "print(model.intercept)"
      ],
      "metadata": {
        "id": "1Nx_oEnPLqLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model\n",
        "model.evaluate(testset).predictions.show(3)"
      ],
      "metadata": {
        "id": "Ta03Qd_LLqOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# handel categorical features with ordinal indexing\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indxr = StringIndexer(inputCol='ChestPainType', outputCol='ChestPainTypeInxed')\n",
        "indxr.fit(df).transform(df).select('ChestPainTypeInxed').show(3)"
      ],
      "metadata": {
        "id": "QB-ogSVzLyGT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}